{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WPILib ML Notebook\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "By using this notebook, you can train a TensorFlow Lite model for use on a Raspberry Pi and Google Coral USB Accelerator. We've designed this process to be as simple as possible. If you find an issue with this notebook, please create a new issue report on our [GitHub page](https://github.com/GrantPerkins/CoralSagemaker), where you downloaded this notebook.\n",
    "\n",
    "### Training\n",
    "\n",
    "1. Download the WPILIB dataset as a .tar file [here](https://github.com/GrantPerkins/CoralSagemaker/releases/download/v1/WPILib.tar)\n",
    "2. Upload your .tar file to a new folder in an Amazon S3 bucket, or a brand new S3 bucket.\n",
    "3. Create a new SageMaker notebook instance, and open the WPILib notebook.\n",
    "4. Change estimator.fit() in the last code cell to use your new dataset, by specifying the folder in which the tar is stored.\n",
    "5. Run the code block.\n",
    "6. Training should take roughly 10 minutes and cost roughly \\\\$0.55 if using the GPU instance, or 45 minutes and cost roughly \\\\$0.45 if using the CPU instance. If you do not change anything in the notebook, other than the S3 location, it should absolutely not take longer than an hour.\n",
    "\n",
    "## Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "# By Grant Perkins, 2019\n",
      "\n",
      "# Make directories used in training\n",
      "mkdir /tensorflow/models/research/learn\n",
      "mkdir /tensorflow/models/research/learn/ckpt\n",
      "\n",
      "# Remove directories created during training\n",
      "rm -rf /tensorflow/models/research/learn/train\n",
      "rm -rf /tensorflow/models/research/learn/models\n",
      "\n",
      "cd /tensorflow/models/research\n",
      "\n",
      "# Download checkpoints for MobileNet v2 hosted by Google\n",
      "echo \"\\nDownloading model\"\n",
      "./prepare_checkpoint_and_dataset.sh --network_type mobilenet_v2_ssd --train_whole_model false  > /dev/null 2>/dev/null\n",
      "\n",
      "./tar_to_record.sh\n",
      "\n",
      "cd /opt/ml/input/data/training\n",
      "classes=$(python3 /tensorflow/models/research/labels.py)\n",
      "cd /tensorflow/models/research\n",
      "\n",
      "sed -i \"s%NUM_CLASSES%${classes}%g\" \"./pipeline.config\"\n",
      "\n",
      "# Copy custom pipeline into docker\n",
      "cp pipeline.config /tensorflow/models/research/learn/ckpt/pipeline.config\n",
      "\n",
      "# Get number of epochs from SageMaker\n",
      "TRAIN_STEPS=$(python3 hyper.py)\n",
      "\n",
      "echo \"Beginning training on Docker image\"\n",
      "./retrain_detection_model.sh --num_training_steps $TRAIN_STEPS --num_eval_steps 1 > /dev/null 2>&1\n",
      "\n",
      "echo \"Converting checkpoint to tflite\"\n",
      "./convert_checkpoint_to_edgetpu_tflite.sh --checkpoint_num $TRAIN_STEPS > /dev/null 2>&1\n",
      "\n",
      "echo \"Compiling model for Edge TPU\"\n",
      "edgetpu_compiler ./learn/models/output_tflite_graph.tflite -o /opt/ml/model/ > /dev/null 2>&1\n",
      "\n",
      "cp /opt/ml/input/data/training/map.pbtxt /opt/ml/model/map.pbtxt\n",
      "\n",
      "rm /opt/ml/model/output_tflite_graph_edgetpu.log\n",
      "\n",
      "mv /opt/ml/model/output_tflite_graph_edgetpu.tflite /opt/ml/model/model.tfliteSending build context to Docker daemon  62.46kB\n",
      "Step 1/18 : FROM tensorflow/tensorflow:1.12.0-rc2-devel-gpu\n",
      " ---> ad9d164e338c\n",
      "Step 2/18 : RUN git clone https://github.com/tensorflow/models.git &&     mv models /tensorflow/models\n",
      " ---> Running in f89faf0640e1\n",
      "\u001b[91mCloning into 'models'...\n",
      "\u001b[0mRemoving intermediate container f89faf0640e1\n",
      " ---> ba057060eefb\n",
      "Step 3/18 : RUN apt-get update -qq > /dev/null &&     apt-get install -y python python-tk python3 python3-pip -qq > /dev/null\n",
      " ---> Running in b7b980fda603\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mRemoving intermediate container b7b980fda603\n",
      " ---> d33d6334ad08\n",
      "Step 4/18 : RUN apt-get update -qq > /dev/null &&     apt-get install -y --no-install-recommends nginx curl -qq > /dev/null\n",
      " ---> Running in 7632fae28844\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mRemoving intermediate container 7632fae28844\n",
      " ---> dbb586022bce\n",
      "Step 5/18 : RUN pip install Cython contextlib2  pillow lxml jupyter matplotlib pandas -q &&     python3 -m pip install pandas -q\n",
      " ---> Running in bac6cc7265c7\n",
      "\u001b[91mYou are using pip version 18.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container bac6cc7265c7\n",
      " ---> 2c68e2e7cdef\n",
      "Step 6/18 : RUN curl -s -OL \"https://github.com/google/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip\" > /dev/null &&     unzip protoc-3.0.0-linux-x86_64.zip -d proto3 > /dev/null &&     mv proto3/bin/* /usr/local/bin &&     mv proto3/include/* /usr/local/include &&     rm -rf proto3 protoc-3.0.0-linux-x86_64.zip\n",
      " ---> Running in eb90c2d7ecca\n",
      "Removing intermediate container eb90c2d7ecca\n",
      " ---> ab7c4e09f661\n",
      "Step 7/18 : RUN git clone --depth 1 https://github.com/cocodataset/cocoapi.git &&     cd cocoapi/PythonAPI &&     make -j8 &&     cp -r pycocotools /tensorflow/models/research &&     cd ../../ &&     rm -rf cocoapi\n",
      " ---> Running in e1f2bfaf32de\n",
      "\u001b[91mCloning into 'cocoapi'...\n",
      "\u001b[0mpython setup.py build_ext --inplace\n",
      "running build_ext\n",
      "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
      "\u001b[91m/usr/local/lib/python2.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /root/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "\u001b[0mbuilding 'pycocotools._mask' extension\n",
      "creating build\n",
      "creating build/common\n",
      "creating build/temp.linux-x86_64-2.7\n",
      "creating build/temp.linux-x86_64-2.7/pycocotools\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/local/lib/python2.7/dist-packages/numpy/core/include -I../common -I/usr/include/python2.7 -c ../common/maskApi.c -o build/temp.linux-x86_64-2.7/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "\u001b[91m../common/maskApi.c: In function 'rleToBbox':\n",
      "../common/maskApi.c:141:31: warning: 'xp' may be used uninitialized in this function [-Wmaybe-uninitialized]\n",
      "       if(j%2==0) xp=x; else if(xp<x) { ys=0; ye=h-1; }\n",
      "                               ^\n",
      "\u001b[0mx86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/local/lib/python2.7/dist-packages/numpy/core/include -I../common -I/usr/include/python2.7 -c pycocotools/_mask.c -o build/temp.linux-x86_64-2.7/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "creating build/lib.linux-x86_64-2.7\n",
      "creating build/lib.linux-x86_64-2.7/pycocotools\n",
      "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-Bsymbolic-functions -Wl,-z,relro -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/../common/maskApi.o build/temp.linux-x86_64-2.7/pycocotools/_mask.o -o build/lib.linux-x86_64-2.7/pycocotools/_mask.so\n",
      "copying build/lib.linux-x86_64-2.7/pycocotools/_mask.so -> pycocotools\n",
      "rm -rf build\n",
      "Removing intermediate container e1f2bfaf32de\n",
      " ---> 4be4510014db\n",
      "Step 8/18 : RUN cd /tensorflow/models/research &&     protoc object_detection/protos/*.proto --python_out=.\n",
      " ---> Running in dd91b69672bc\n",
      "Removing intermediate container dd91b69672bc\n",
      " ---> 24056228c207\n",
      "Step 9/18 : ENV PYTHONPATH $PYTHONPATH:/tensorflow/models/research:/tensorflow/models/research/slim\n",
      " ---> Running in 0a0b66b825b0\n",
      "Removing intermediate container 0a0b66b825b0\n",
      " ---> ee2b6dd8ce6e\n",
      "Step 10/18 : RUN apt-get update -qq > /dev/null &&     apt-get install -y wget vim emacs nano -qq > /dev/null\n",
      " ---> Running in dbd569675155\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mRemoving intermediate container dbd569675155\n",
      " ---> 74d59523a5c9\n",
      "Step 11/18 : ARG work_dir=/tensorflow/models/research\n",
      " ---> Running in 3f7904190004\n",
      "Removing intermediate container 3f7904190004\n",
      " ---> 8a8af68e098f\n",
      "Step 12/18 : ARG scripts_link=\"http://storage.googleapis.com/cloud-iot-edge-pretrained-models/docker/obj_det_scripts.tgz\"\n",
      " ---> Running in 412f7b52922e\n",
      "Removing intermediate container 412f7b52922e\n",
      " ---> c0756af23115\n",
      "Step 13/18 : RUN cd ${work_dir} &&     wget -q -O obj_det_scripts.tgz ${scripts_link} &&     tar zxf obj_det_scripts.tgz\n",
      " ---> Running in 288ed79892ab\n",
      "Removing intermediate container 288ed79892ab\n",
      " ---> 3fc038c4676c\n",
      "Step 14/18 : RUN apt-get install -y apt-transport-https ca-certificates\n",
      " ---> Running in 44ed2eda9140\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following packages will be upgraded:\n",
      "  apt-transport-https ca-certificates\n",
      "2 upgraded, 0 newly installed, 0 to remove and 102 not upgraded.\n",
      "Need to get 194 kB of archives.\n",
      "After this operation, 1024 B of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 apt-transport-https amd64 1.2.32 [26.5 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 ca-certificates all 20170717~16.04.2 [167 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 194 kB in 0s (235 kB/s)\n",
      "(Reading database ... 42117 files and directories currently installed.)\n",
      "Preparing to unpack .../apt-transport-https_1.2.32_amd64.deb ...\n",
      "Unpacking apt-transport-https (1.2.32) over (1.2.27) ...\n",
      "Preparing to unpack .../ca-certificates_20170717~16.04.2_all.deb ...\n",
      "Unpacking ca-certificates (20170717~16.04.2) over (20170717~16.04.1) ...\n",
      "Setting up apt-transport-https (1.2.32) ...\n",
      "Setting up ca-certificates (20170717~16.04.2) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Processing triggers for ca-certificates (20170717~16.04.2) ...\n",
      "Updating certificates in /etc/ssl/certs...\n",
      "0 added, 0 removed; done.\n",
      "Running hooks in /etc/ca-certificates/update.d...\n",
      "done.\n",
      "Removing intermediate container 44ed2eda9140\n",
      " ---> 18999a5618a1\n",
      "Step 15/18 : RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&     echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | tee /etc/apt/sources.list.d/coral-edgetpu.list &&     apt-get update &&     apt-get install -y edgetpu\n",
      " ---> Running in ae68125aa4fe\n",
      "\u001b[91m  \u001b[0m\u001b[91m% Total \u001b[0m\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91m % R\u001b[0m\u001b[91me\u001b[0m\u001b[91mce\u001b[0m\u001b[91mi\u001b[0m\u001b[91mv\u001b[0m\u001b[91me\u001b[0m\u001b[91md \u001b[0m\u001b[91m%\u001b[0m\u001b[91m \u001b[0m\u001b[91mXfe\u001b[0m\u001b[91mr\u001b[0m\u001b[91md\u001b[0m\u001b[91m \u001b[0m\u001b[91m Average Sp\u001b[0m\u001b[91me\u001b[0m\u001b[91me\u001b[0m\u001b[91md \u001b[0m\u001b[91m \u001b[0m\u001b[91m Ti\u001b[0m\u001b[91mm\u001b[0m\u001b[91me\u001b[0m\u001b[91m  \u001b[0m\u001b[91m \u001b[0m\u001b[91m T\u001b[0m\u001b[91mim\u001b[0m\u001b[91me \u001b[0m\u001b[91m \u001b[0m\u001b[91m  \u001b[0m\u001b[91m T\u001b[0m\u001b[91mime\u001b[0m\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91mCu\u001b[0m\u001b[91mrr\u001b[0m\u001b[91me\u001b[0m\u001b[91mn\u001b[0m\u001b[91mt\n",
      "\u001b[0m\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91m  \u001b[0m\u001b[91m  \u001b[0m\u001b[91m  \u001b[0m\u001b[91m     \u001b[0m\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91m  \u001b[0m\u001b[91m          D\u001b[0m\u001b[91mlo\u001b[0m\u001b[91mad\u001b[0m\u001b[91m  \u001b[0m\u001b[91mUp\u001b[0m\u001b[91ml\u001b[0m\u001b[91moad   Total   Spent    Left  Speed\n",
      "100   659  100   659    0     0   5582      0 --:--:-- --:--:-- --:--:--  5632\u001b[0m\u001b[91m\n",
      "\u001b[0mOK\n",
      "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\n",
      "Get:1 file:/var/nvinfer-runtime-trt-repo-4.0.1-ga-cuda9.0  InRelease\n",
      "Ign:1 file:/var/nvinfer-runtime-trt-repo-4.0.1-ga-cuda9.0  InRelease\n",
      "Get:2 file:/var/nvinfer-runtime-trt-repo-4.0.1-ga-cuda9.0  Release [574 B]\n",
      "Get:2 file:/var/nvinfer-runtime-trt-repo-4.0.1-ga-cuda9.0  Release [574 B]\n",
      "Hit:3 http://security.ubuntu.com/ubuntu xenial-security InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu xenial InRelease\n",
      "Ign:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  InRelease\n",
      "Ign:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  InRelease\n",
      "Hit:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release\n",
      "Hit:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release\n",
      "Hit:10 http://archive.ubuntu.com/ubuntu xenial-updates InRelease\n",
      "Get:12 https://packages.cloud.google.com/apt coral-edgetpu-stable InRelease [6332 B]\n",
      "Hit:14 http://archive.ubuntu.com/ubuntu xenial-backports InRelease\n",
      "Get:15 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 Packages [1376 B]\n",
      "Fetched 7708 B in 0s (12.7 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  edgetpu-compiler libedgetpu1-std\n",
      "The following NEW packages will be installed:\n",
      "  edgetpu edgetpu-compiler libedgetpu1-std\n",
      "0 upgraded, 3 newly installed, 0 to remove and 102 not upgraded.\n",
      "Need to get 4157 kB of archives.\n",
      "After this operation, 14.8 MB of additional disk space will be used.\n",
      "Get:1 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 edgetpu-compiler amd64 12-1 [3842 kB]\n",
      "Get:2 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 libedgetpu1-std amd64 12-1 [312 kB]\n",
      "Get:3 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 edgetpu all 12-1 [3232 B]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 4157 kB in 0s (7017 kB/s)\n",
      "Selecting previously unselected package edgetpu-compiler.\n",
      "(Reading database ... 42117 files and directories currently installed.)\n",
      "Preparing to unpack .../edgetpu-compiler_12-1_amd64.deb ...\n",
      "Unpacking edgetpu-compiler (12-1) ...\n",
      "Selecting previously unselected package libedgetpu1-std:amd64.\n",
      "Preparing to unpack .../libedgetpu1-std_12-1_amd64.deb ...\n",
      "Unpacking libedgetpu1-std:amd64 (12-1) ...\n",
      "Selecting previously unselected package edgetpu.\n",
      "Preparing to unpack .../archives/edgetpu_12-1_all.deb ...\n",
      "Unpacking edgetpu (12-1) ...\n",
      "Processing triggers for libc-bin (2.23-0ubuntu10) ...\n",
      "Setting up edgetpu-compiler (12-1) ...\n",
      "Setting up libedgetpu1-std:amd64 (12-1) ...\n",
      "Setting up edgetpu (12-1) ...\n",
      "Processing triggers for libc-bin (2.23-0ubuntu10) ...\n",
      "Removing intermediate container ae68125aa4fe\n",
      " ---> 644f3c56dda2\n",
      "Step 16/18 : COPY /coral /tensorflow/models/research\n",
      " ---> 7f265d91380d\n",
      "Step 17/18 : ENV PATH $PATH:/tensorflow/models/research\n",
      " ---> Running in 09965929d1eb\n",
      "Removing intermediate container 09965929d1eb\n",
      " ---> cbe2245fff1f\n",
      "Step 18/18 : WORKDIR ${work_dir}\n",
      " ---> Running in 123f158d5f93\n",
      "Removing intermediate container 123f158d5f93\n",
      " ---> 0d16fa0b0bc3\n",
      "Successfully built 0d16fa0b0bc3\n",
      "Successfully tagged wpi-gpu:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Could not connect to the endpoint URL: \"https://api.ecr.1.amazonaws.com/\"\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "#!/usr/bin/env bash\n",
    "docker system prune --force > /dev/null 2>&1\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=wpi-gpu\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod -R +x coral/\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:us-east-1}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email) 2> /dev/null\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} . --no-cache # > /dev/null 2>&1\n",
    "docker tag ${algorithm_name} ${fullname} > /dev/null 2>&1\n",
    "\n",
    "docker push ${fullname} > /dev/null 2>&1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This step runs the training instance (default for GPU is a ml.p3.2xlarge and for the default is CPU is an ml.c4.2xlarge), and begins training with the data specified in `fit()`\n",
    "\n",
    "This section has lots of configurable values\n",
    "You need to change `estimator.fit(...)`:to be the location of the data used for training. (the bucket you uploaded the .tar to) It should be in the format `\"s3://BUCKET-NAME\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249838237784.dkr.ecr.us-east-1.amazonaws.com/wpi-gpu:latest\n",
      "2019-11-04 05:01:56 Starting - Starting the training job...\n",
      "2019-11-04 05:01:57 Starting - Launching requested ML instances......\n",
      "2019-11-04 05:03:04 Starting - Preparing the instances for training...\n",
      "2019-11-04 05:03:55 Downloading - Downloading input data...\n",
      "2019-11-04 05:04:09 Training - Downloading the training image............\n",
      "2019-11-04 05:06:12 Training - Training image download completed. Training in progress.\u001b[31mDownloading model\u001b[0m\n",
      "\u001b[31mSuccessfully created the TFRecords: /opt/ml/input/data/training/train.record\u001b[0m\n",
      "\u001b[31mSuccessfully created the TFRecords: /opt/ml/input/data/training/eval.record\u001b[0m\n",
      "\u001b[31mRecords generated.\u001b[0m\n",
      "\u001b[31mBeginning training on Docker image\u001b[0m\n",
      "\u001b[31m+ num_training_steps=500\u001b[0m\n",
      "\u001b[31m+ [[ 4 -gt 0 ]]\u001b[0m\n",
      "\u001b[31m+ case \"$1\" in\u001b[0m\n",
      "\u001b[31m+ num_training_steps=500\u001b[0m\n",
      "\u001b[31m+ shift 2\u001b[0m\n",
      "\u001b[31m+ [[ 2 -gt 0 ]]\u001b[0m\n",
      "\u001b[31m+ case \"$1\" in\u001b[0m\n",
      "\u001b[31m+ num_eval_steps=1\u001b[0m\n",
      "\u001b[31m+ shift 2\u001b[0m\n",
      "\u001b[31m+ [[ 0 -gt 0 ]]\u001b[0m\n",
      "\u001b[31m+ source /tensorflow/models/research/constants.sh\u001b[0m\n",
      "\u001b[31m++ declare -A ckpt_link_map\u001b[0m\n",
      "\u001b[31m++ declare -A ckpt_name_map\u001b[0m\n",
      "\u001b[31m++ declare -A config_filename_map\u001b[0m\n",
      "\u001b[31m++ ckpt_link_map[\"mobilenet_v1_ssd\"]=http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz\u001b[0m\n",
      "\u001b[31m++ ckpt_link_map[\"mobilenet_v2_ssd\"]=http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\u001b[0m\n",
      "\u001b[31m++ ckpt_name_map[\"mobilenet_v1_ssd\"]=ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18\u001b[0m\n",
      "\u001b[31m++ ckpt_name_map[\"mobilenet_v2_ssd\"]=ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03\u001b[0m\n",
      "\u001b[31m++ config_filename_map[\"mobilenet_v1_ssd-true\"]=pipeline_mobilenet_v1_ssd_retrain_whole_model.config\u001b[0m\n",
      "\u001b[31m++ config_filename_map[\"mobilenet_v1_ssd-false\"]=pipeline_mobilenet_v1_ssd_retrain_last_few_layers.config\u001b[0m\n",
      "\u001b[31m++ config_filename_map[\"mobilenet_v2_ssd-true\"]=pipeline_mobilenet_v2_ssd_retrain_whole_model.config\u001b[0m\n",
      "\u001b[31m++ config_filename_map[\"mobilenet_v2_ssd-false\"]=pipeline_mobilenet_v2_ssd_retrain_last_few_layers.config\u001b[0m\n",
      "\u001b[31m++ INPUT_TENSORS=normalized_input_image_tensor\u001b[0m\n",
      "\u001b[31m++ OUTPUT_TENSORS=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\u001b[0m\n",
      "\u001b[31m++ OBJ_DET_DIR=/tensorflow/models/research\u001b[0m\n",
      "\u001b[31m++ LEARN_DIR=/tensorflow/models/research/learn\u001b[0m\n",
      "\u001b[31m++ CKPT_DIR=/tensorflow/models/research/learn/ckpt\u001b[0m\n",
      "\u001b[31m++ TRAIN_DIR=/tensorflow/models/research/learn/train\u001b[0m\n",
      "\u001b[31m++ OUTPUT_DIR=/tensorflow/models/research/learn/models\u001b[0m\n",
      "\u001b[31m+ mkdir /tensorflow/models/research/learn/train\u001b[0m\n",
      "\u001b[31m+ python object_detection/model_main.py --pipeline_config_path=/tensorflow/models/research/learn/ckpt/pipeline.config --model_dir=/tensorflow/models/research/learn/train --num_train_steps=500 --num_eval_steps=1\u001b[0m\n",
      "\u001b[31m/tensorflow/models/research/object_detection/utils/visualization_utils.py:29: UserWarning: \u001b[0m\n",
      "\u001b[31mThis call to matplotlib.use() has no effect because the backend has already\u001b[0m\n",
      "\u001b[31mbeen chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\u001b[0m\n",
      "\u001b[31mor matplotlib.backends is imported for the first time.\n",
      "\u001b[0m\n",
      "\u001b[31mThe backend was *originally* set to 'TkAgg' by the following code:\n",
      "  File \"object_detection/model_main.py\", line 26, in <module>\n",
      "    from object_detection import model_lib\n",
      "  File \"/tensorflow/models/research/object_detection/model_lib.py\", line 27, in <module>\n",
      "    from object_detection import eval_util\n",
      "  File \"/tensorflow/models/research/object_detection/eval_util.py\", line 33, in <module>\n",
      "    from object_detection.metrics import coco_evaluation\n",
      "  File \"/tensorflow/models/research/object_detection/metrics/coco_evaluation.py\", line 25, in <module>\n",
      "    from object_detection.metrics import coco_tools\n",
      "  File \"/tensorflow/models/research/object_detection/metrics/coco_tools.py\", line 51, in <module>\n",
      "    from pycocotools import coco\n",
      "  File \"/tensorflow/models/research/pycocotools/coco.py\", line 49, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7f43411aa578>) includes params argument, but params are not passed to Estimator.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /tensorflow/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mUse `tf.data.experimental.parallel_interleave(...)`.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /tensorflow/models/research/object_detection/builders/dataset_builder.py:158: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mUse `tf.data.Dataset.batch(..., drop_remainder=True)`.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_0/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[273]], model variable shape: [[24]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_0/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 576, 273]], model variable shape: [[1, 1, 576, 24]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_1/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_1/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 1280, 546]], model variable shape: [[1, 1, 1280, 48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_2/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_2/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 546]], model variable shape: [[1, 1, 512, 48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_3/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_3/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_4/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_4/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_5/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_5/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 546]], model variable shape: [[1, 1, 128, 48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [global_step] is not available in checkpoint\u001b[0m\n",
      "\u001b[31m2019-11-04 05:07:18.262791: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\u001b[0m\n",
      "\u001b[31m2019-11-04 05:07:18.470989: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:964] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\u001b[0m\n",
      "\u001b[31m2019-11-04 05:07:18.471950: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1432] Found device 0 with properties: \u001b[0m\n",
      "\u001b[31mname: Tesla V100-SXM2-16GB major: 7 minor: 0 memoryClockRate(GHz): 1.53\u001b[0m\n",
      "\u001b[31mpciBusID: 0000:00:1e.0\u001b[0m\n",
      "\u001b[31mtotalMemory: 15.75GiB freeMemory: 15.44GiB\u001b[0m\n",
      "\u001b[31m2019-11-04 05:07:18.471982: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1511] Adding visible gpu devices: 0\u001b[0m\n",
      "\u001b[31m2019-11-04 05:07:20.497802: I tensorflow/core/common_runtime/gpu/gpu_device.cc:982] Device interconnect StreamExecutor with strength 1 edge matrix:\u001b[0m\n",
      "\u001b[31m2019-11-04 05:07:20.497862: I tensorflow/core/common_runtime/gpu/gpu_device.cc:988]      0 \u001b[0m\n",
      "\u001b[31m2019-11-04 05:07:20.497873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1001] 0:   N \u001b[0m\n",
      "\u001b[31m2019-11-04 05:07:20.498038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1115] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14940 MB memory) -> physical GPU (device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0)\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e2ca15bc2893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Change this bucket if you want to train with your own data. The WPILib bucket contains thousands of high quality labeled images.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# s3://wpilib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3://wpilib\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name)\u001b[0m\n\u001b[1;32m    339\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 341\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \"\"\"\n\u001b[1;32m    903\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll)\u001b[0m\n\u001b[1;32m   1493\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1494\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1495\u001b[0;31m             \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1496\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1497\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mLogState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mJOB_COMPLETE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "# Uses GPU by default, change to false to use CPU\n",
    "use_gpu = True\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "instance_type = None\n",
    "algorithm_name = None\n",
    "\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name\n",
    "\n",
    "if not use_gpu:\n",
    "    instance_type = 'ml.c4.2xlarge'\n",
    "    algorithm_name = 'sagemaker-tf-wpi'\n",
    "else:\n",
    "    instance_type = 'ml.p3.2xlarge'\n",
    "    algorithm_name = 'wpi-gpu'\n",
    "\n",
    "# The number of epochs to train to. 500 is a safe number. With the default instance, it should take 45 minutes.\n",
    "hyperparameters = {'epochs': 500}\n",
    "\n",
    "ecr_image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)\n",
    "print(ecr_image)\n",
    "\n",
    "# The estimator object, using our notebook, training instance, the ECR image, and the specified training steps\n",
    "estimator = Estimator(role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=instance_type,\n",
    "                      image_name=ecr_image,\n",
    "                      hyperparameters=hyperparameters)\n",
    "\n",
    "# Change this bucket if you want to train with your own data. The WPILib bucket contains thousands of high quality labeled images.\n",
    "# s3://wpilib\n",
    "estimator.fit(\"s3://wpilib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Running inference on your trained model.\n",
    "\n",
    "1. After running all of the code in this notebook, go to the training job in SageMaker, scroll to the bottom, and find the output S3 location\n",
    "2. Download the the .tar file in the bucket, extract it, and get your .tflite file\n",
    "3. FTP output.tflite into the directory SD_CARD:/home/pi.\n",
    "4. Run the python script, using `python3 object_detection.py --team 190`\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
