{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WPILib ML Notebook\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "By using this notebook, you can train a TensorFlow Lite model for use on a Raspberry Pi and Google Coral USB Accelerator. We've designed this process to be as simple as possible. If you find an issue with this notebook, please create a new issue report on our GitHub page, where you downloaded this notebook.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "This section will explain the four distinct steps to getting a trained model to run on your hardware.\n",
    "### Getting Data\n",
    "\n",
    "WPILib provides thousands of labelled images for this years game, which you can download here. However, you can train with custom data using this notebook as well. The below instructions describe how to gather and label your own data.\n",
    "\n",
    "1. Plug a USB Camera into your laptop, and run a script similar to record_video.py, which simply makes an mp4 from the camera stream.\n",
    "2. Create a [supervise.ly](supervise.ly) account. This is a very nice tool for labelling data.\n",
    "3. (Optional) You can add other teammates to your Supervise.ly workspace by clicking 'Members' on the left and then 'INVITE' at the top.\n",
    "4. Choose a workspace to work in, in the 'Workspaces' tab.\n",
    "5. Upload the official WPILib labelled data to your workspace. Download the tar here, extract it, then click 'IMPORT DATA' or 'UPLOAD' inside of your workspace. Change the import plugin to Supervisely, then drag in the extracted FOLDER. Then, give the project a name, then click import.\n",
    "6. Upload your own video to your workspace. Click 'UPLOAD' when inside of your workspace, change your import plugin to video, drag in your video, give the project a name, and click import.\n",
    "7. Click into your newly import Dataset. Use the rectangle tool to draw appropriate boxes around the objects which you wish to label.\n",
    "\n",
    "### Training\n",
    "\n",
    "1. Download your datasets from Supervise.ly. Select the \"json and jpeg\" option.\n",
    "2. Upload your tar to a new folder in an Amazon S3 bucket, or a brand new S3 bucket.\n",
    "3. Create a new SageMaker notebook instance, and open the WPILib notebook.\n",
    "4. Change estimator.fit() to use your new dataset, by specifying the folder in which the tar is stored.\n",
    "5. Run each block of the notebook in order.\n",
    "6. Training should take roughly 45 minutes. If you do not change anything in the notebook, it should absolutely not take longer than an hour. In the Training Job, CPU usage should be around 690 during the majority of the training, if running on an ml.c4.2xlarge. If it is less, something went wrong.\n",
    "\n",
    "### Inference\n",
    "\n",
    "1. Go to the training job in SageMaker, scroll to the bottom, and find the output S3 location\n",
    "2. Download the the tar file in the bucket, extract it, and get your .tflite file\n",
    "3. Put the tflite on your Raspberry Pi by plugging in the SD card into your computer and dragging it in to /home/pi\n",
    "4. Run the python script, using `python3 object_detection.py --model output.tflite`\n",
    "\n",
    "\n",
    "## Notebook\n",
    "### Building and registering the container\n",
    "\n",
    "This code block runs a script that builds a docker container, and saves it as an Amazon ECR image. This image is used by the training instance so that all proper dependencies and WPILib files are in place.\n",
    "\n",
    "Just run this one, don't change anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "#!/usr/bin/env bash\n",
    "docker system prune --force > /dev/null 2>&1\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-tf-wpi\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod -R +x coral/\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email) 2> /dev/null\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} . --no-cache > /dev/null 2>&1\n",
    "docker tag ${algorithm_name} ${fullname} > /dev/null 2>&1\n",
    "\n",
    "docker push ${fullname} > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get execution role\n",
    "\n",
    "This gets the notebook instance's execution role, used for communicating with the training instance.\n",
    "\n",
    "Just run this one too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on SageMaker\n",
    "Training a model on SageMaker with the Python SDK is done in a way that is similar to the way we trained it locally. This is done by changing our train_instance_type from `local` to one of our [supported EC2 instance types](https://aws.amazon.com/sagemaker/pricing/instance-types/).\n",
    "\n",
    "In addition, we must now specify the ECR image URL, which we just pushed above.\n",
    "\n",
    "Just run this one too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "249838237784.dkr.ecr.us-east-1.amazonaws.com/sagemaker-tf-wpi:latest\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name\n",
    "\n",
    "\n",
    "# If you want to change the algorithm name, make sure to change the name in the first block too.\n",
    "algorithm_name = 'sagemaker-tf-wpi'\n",
    "\n",
    "ecr_image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)\n",
    "\n",
    "print(ecr_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This last step runs the training instance (default an ml.c4.2xlarge), and begins training with the data specified in `fit()`\n",
    "\n",
    "This section has lots of values worth changing\n",
    "`instance_type`: change the type of computer that the training will run on.\n",
    "`hyperparameters`: change the number of epochs.\n",
    "`estimator.fit(...)`: change the location of the data used for training. should be in the format `\"s3://BUCKET-NAME\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-09-19 00:22:09 Starting - Starting the training job...\n",
      "2019-09-19 00:22:12 Starting - Launching requested ML instances......\n",
      "2019-09-19 00:23:13 Starting - Preparing the instances for training...\n",
      "2019-09-19 00:24:10 Downloading - Downloading input data\n",
      "2019-09-19 00:24:10 Training - Downloading the training image.........\n",
      "2019-09-19 00:25:36 Training - Training image download completed. Training in progress..\u001b[31m+ set -e\u001b[0m\n",
      "\u001b[31m+ mkdir /tensorflow/models/research/learn\u001b[0m\n",
      "\u001b[31m+ mkdir /tensorflow/models/research/learn/ckpt\u001b[0m\n",
      "\u001b[31m+ rm -rf /tensorflow/models/research/learn/train\u001b[0m\n",
      "\u001b[31m+ rm -rf /tensorflow/models/research/learn/models\u001b[0m\n",
      "\u001b[31m+ cd /tensorflow/models/research\u001b[0m\n",
      "\u001b[31m+ echo 'Downloading model'\u001b[0m\n",
      "\u001b[31mDownloading model\u001b[0m\n",
      "\u001b[31m+ ./prepare_checkpoint_and_dataset.sh --network_type mobilenet_v2_ssd --train_whole_model false\u001b[0m\n",
      "\u001b[31m+ network_type=mobilenet_v2_ssd\u001b[0m\n",
      "\u001b[31m+ train_whole_model=false\u001b[0m\n",
      "\u001b[31m+ [[ 4 -gt 0 ]]\u001b[0m\n",
      "\u001b[31m+ case \"$1\" in\u001b[0m\n",
      "\u001b[31m+ network_type=mobilenet_v2_ssd\u001b[0m\n",
      "\u001b[31m+ shift 2\u001b[0m\n",
      "\u001b[31m+ [[ 2 -gt 0 ]]\u001b[0m\n",
      "\u001b[31m+ case \"$1\" in\u001b[0m\n",
      "\u001b[31m+ train_whole_model=false\u001b[0m\n",
      "\u001b[31m+ shift 2\u001b[0m\n",
      "\u001b[31m+ [[ 0 -gt 0 ]]\u001b[0m\n",
      "\u001b[31m+ source /tensorflow/models/research/constants.sh\u001b[0m\n",
      "\u001b[31m++ declare -A ckpt_link_map\u001b[0m\n",
      "\u001b[31m++ declare -A ckpt_name_map\u001b[0m\n",
      "\u001b[31m++ declare -A config_filename_map\u001b[0m\n",
      "\u001b[31m++ ckpt_link_map[\"mobilenet_v1_ssd\"]=http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz\u001b[0m\n",
      "\u001b[31m++ ckpt_link_map[\"mobilenet_v2_ssd\"]=http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\u001b[0m\n",
      "\u001b[31m++ ckpt_name_map[\"mobilenet_v1_ssd\"]=ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18\u001b[0m\n",
      "\u001b[31m++ ckpt_name_map[\"mobilenet_v2_ssd\"]=ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03\u001b[0m\n",
      "\u001b[31m++ config_filename_map[\"mobilenet_v1_ssd-true\"]=pipeline_mobilenet_v1_ssd_retrain_whole_model.config\u001b[0m\n",
      "\u001b[31m++ config_filename_map[\"mobilenet_v1_ssd-false\"]=pipeline_mobilenet_v1_ssd_retrain_last_few_layers.config\u001b[0m\n",
      "\u001b[31m++ config_filename_map[\"mobilenet_v2_ssd-true\"]=pipeline_mobilenet_v2_ssd_retrain_whole_model.config\u001b[0m\n",
      "\u001b[31m++ config_filename_map[\"mobilenet_v2_ssd-false\"]=pipeline_mobilenet_v2_ssd_retrain_last_few_layers.config\u001b[0m\n",
      "\u001b[31m++ INPUT_TENSORS=normalized_input_image_tensor\u001b[0m\n",
      "\u001b[31m++ OUTPUT_TENSORS=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\u001b[0m\n",
      "\u001b[31m++ OBJ_DET_DIR=/tensorflow/models/research\u001b[0m\n",
      "\u001b[31m++ LEARN_DIR=/tensorflow/models/research/learn\u001b[0m\n",
      "\u001b[31m++ CKPT_DIR=/tensorflow/models/research/learn/ckpt\u001b[0m\n",
      "\u001b[31m++ TRAIN_DIR=/tensorflow/models/research/learn/train\u001b[0m\n",
      "\u001b[31mPreparing checkpoint\u001b[0m\n",
      "\u001b[31m++ OUTPUT_DIR=/tensorflow/models/research/learn/models\u001b[0m\n",
      "\u001b[31m+ echo 'Preparing checkpoint'\u001b[0m\n",
      "\u001b[31m+ mkdir -p /tensorflow/models/research/learn\u001b[0m\n",
      "\u001b[31m+ ckpt_link=http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\u001b[0m\n",
      "\u001b[31m+ ckpt_name=ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03\u001b[0m\n",
      "\u001b[31m+ cd /tensorflow/models/research/learn\u001b[0m\n",
      "\u001b[31m+ wget -q -O ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\u001b[0m\n",
      "\u001b[31m+ tar zxf ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\u001b[0m\n",
      "\u001b[31m+ mv ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03 /tensorflow/models/research/learn/ckpt\u001b[0m\n",
      "\u001b[31m+ ./tar_to_record.sh\u001b[0m\n",
      "\u001b[31m+ cd /opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31m+ rm -rf out\u001b[0m\n",
      "\u001b[31m+ mkdir out\u001b[0m\n",
      "\u001b[31m+ mkdir tmp\u001b[0m\n",
      "\u001b[31m++ find . -name Mark.tar\u001b[0m\n",
      "\u001b[31m+ tar_file=./Mark.tar\u001b[0m\n",
      "\u001b[31m+ tar -xf ./Mark.tar -C ./out/\u001b[0m\n",
      "\u001b[31m+ python3 /tensorflow/models/research/json_to_csv.py\u001b[0m\n",
      "\u001b[31m+ python /tensorflow/models/research/generate_tfrecord.py --input_csv=./tmp/train.csv --output_tfrecord=train.record\u001b[0m\n",
      "\u001b[31mSuccessfully created the TFRecords: /opt/ml/input/data/training/train.record\u001b[0m\n",
      "\u001b[31m+ python /tensorflow/models/research/generate_tfrecord.py --input_csv=./tmp/eval.csv --output_tfrecord=eval.record\u001b[0m\n",
      "\u001b[31mSuccessfully created the TFRecords: /opt/ml/input/data/training/eval.record\u001b[0m\n",
      "\u001b[31m+ python3 /tensorflow/models/research/parse_meta.py -out=map.pbtxt\u001b[0m\n",
      "\u001b[31m+ echo 'Records generated.'\u001b[0m\n",
      "\u001b[31mRecords generated.\u001b[0m\n",
      "\u001b[31m+ cd /opt/ml/input/data/training\u001b[0m\n",
      "\u001b[31m++ python3 /tensorflow/models/research/labels.py\u001b[0m\n",
      "\u001b[31m+ classes=7\u001b[0m\n",
      "\u001b[31m+ cd /tensorflow/models/research\u001b[0m\n",
      "\u001b[31m+ sed -i s%NUM_CLASSES%7%g ./pipeline.config\u001b[0m\n",
      "\u001b[31m+ cat pipeline.config\u001b[0m\n",
      "\u001b[31m# Quantized trained SSD with Mobilenet v2\u001b[0m\n",
      "\u001b[31m# Only change values in here if you know what you're doing.\u001b[0m\n",
      "\u001b[31m# num_steps is overwritten by the retraining script.\n",
      "\u001b[0m\n",
      "\u001b[31m+ pwd\u001b[0m\n",
      "\u001b[31m+ ls /tensorflow/models/research/learn/ckpt/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/\u001b[0m\n",
      "\u001b[31mmodel {\n",
      "  ssd {\n",
      "    num_classes: 7\n",
      "    image_resizer {\n",
      "      fixed_shape_resizer {\n",
      "        height: 300\n",
      "        width: 300\n",
      "      }\n",
      "    }\n",
      "    feature_extractor {\n",
      "      type: \"ssd_mobilenet_v2\"\n",
      "      depth_multiplier: 1.0\n",
      "      min_depth: 16\n",
      "      conv_hyperparams {\n",
      "        regularizer {\n",
      "          l2_regularizer {\n",
      "            weight: 3.99999989895e-05\n",
      "          }\n",
      "        }\n",
      "        initializer {\n",
      "          random_normal_initializer {\n",
      "            mean: 0.0\n",
      "            stddev: 0.00999999977648\n",
      "          }\n",
      "        }\n",
      "        activation: RELU_6\n",
      "        batch_norm {\n",
      "          decay: 0.97000002861\n",
      "          center: true\n",
      "          scale: true\n",
      "          epsilon: 0.0010000000475\n",
      "        }\n",
      "      }\n",
      "      override_base_feature_extractor_hyperparams: true\n",
      "    }\n",
      "    box_coder {\n",
      "      faster_rcnn_box_coder {\n",
      "        y_scale: 10.0\n",
      "        x_scale: 10.0\n",
      "        height_scale: 5.0\n",
      "        width_scale: 5.0\n",
      "      }\n",
      "    }\n",
      "    matcher {\n",
      "      argmax_matcher {\n",
      "        matched_threshold: 0.5\n",
      "        unmatched_threshold: 0.5\n",
      "        ignore_thresholds: false\n",
      "        negatives_lower_than_unmatched: true\n",
      "        force_match_for_each_row: true\n",
      "        use_matmul_gather: true\n",
      "      }\n",
      "    }\n",
      "    similarity_calculator {\n",
      "      iou_similarity {\n",
      "      }\n",
      "    }\n",
      "    box_predictor {\n",
      "      convolutional_box_predictor {\n",
      "        conv_hyperparams {\n",
      "          regularizer {\n",
      "            l2_regularizer {\n",
      "              weight: 3.99999989895e-05\n",
      "            }\n",
      "          }\n",
      "          initializer {\n",
      "            random_normal_initializer {\n",
      "              mean: 0.0\n",
      "              stddev: 0.00999999977648\n",
      "            }\n",
      "          }\n",
      "          activation: RELU_6\n",
      "          batch_norm {\n",
      "            decay: 0.97000002861\n",
      "            center: true\n",
      "            scale: true\n",
      "            epsilon: 0.0010000000475\n",
      "          }\n",
      "        }\n",
      "        min_depth: 0\n",
      "        max_depth: 0\n",
      "        num_layers_before_predictor: 0\n",
      "        use_dropout: false\n",
      "        dropout_keep_probability: 0.800000011921\n",
      "        kernel_size: 1\n",
      "        box_code_size: 4\n",
      "        apply_sigmoid_to_scores: false\n",
      "        class_prediction_bias_init: -4.59999990463\n",
      "      }\n",
      "    }\n",
      "    anchor_generator {\n",
      "      ssd_anchor_generator {\n",
      "        num_layers: 6\n",
      "        min_scale: 0.20000000298\n",
      "        max_scale: 0.949999988079\n",
      "        aspect_ratios: 1.0\n",
      "        aspect_ratios: 2.0\n",
      "        aspect_ratios: 0.5\n",
      "        aspect_ratios: 3.0\n",
      "        aspect_ratios: 0.333299994469\n",
      "      }\n",
      "    }\n",
      "    post_processing {\n",
      "      batch_non_max_suppression {\n",
      "        score_threshold: 0.300000011921\n",
      "        iou_threshold: 0.600000023842\n",
      "        max_detections_per_class: 100\n",
      "        max_total_detections: 100\n",
      "      }\n",
      "      score_converter: SIGMOID\n",
      "    }\n",
      "    normalize_loss_by_num_matches: true\n",
      "    loss {\n",
      "      localization_loss {\n",
      "        weighted_smooth_l1 {\n",
      "        }\n",
      "      }\n",
      "      classification_loss {\n",
      "        weighted_sigmoid_focal {\n",
      "          gamma: 2.0\n",
      "          alpha: 0.75\n",
      "        }\n",
      "      }\n",
      "      classification_weight: 1.0\n",
      "      localization_weight: 1.0\n",
      "    }\n",
      "    encode_background_as_zeros: true\n",
      "    normalize_loc_loss_by_codesize: true\n",
      "    inplace_batchnorm_update: true\n",
      "    freeze_batchnorm: false\n",
      "  }\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31mtrain_config {\n",
      "  batch_size: 32\n",
      "  data_augmentation_options {\n",
      "    random_horizontal_flip {\n",
      "    }\n",
      "  }\n",
      "  data_augmentation_options {\n",
      "    ssd_random_crop {\n",
      "    }\n",
      "  }\n",
      "  sync_replicas: true\n",
      "  optimizer {\n",
      "    momentum_optimizer {\n",
      "      learning_rate {\n",
      "        cosine_decay_learning_rate {\n",
      "          learning_rate_base: 0.20000000298\n",
      "          total_steps: 1000\n",
      "          warmup_learning_rate: 0.0599999986589\n",
      "          warmup_steps: 100\n",
      "        }\n",
      "      }\n",
      "      momentum_optimizer_value: 0.899999976158\n",
      "    }\n",
      "    use_moving_average: false\n",
      "  }\n",
      "  fine_tune_checkpoint: \"/tensorflow/models/research/learn/ckpt/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03/model.ckpt\"\n",
      "  from_detection_checkpoint: true\n",
      "  load_all_detection_checkpoint_vars: true\n",
      "  num_steps: 50000\n",
      "  startup_delay_steps: 0.0\n",
      "  replicas_to_aggregate: 8\n",
      "  max_number_of_boxes: 100\n",
      "  unpad_groundtruth_tensors: false\n",
      "  freeze_variables:\n",
      "        [ 'FeatureExtractor/MobilenetV2/Conv/',\n",
      "          'FeatureExtractor/MobilenetV2/expanded_conv/',\n",
      "          'FeatureExtractor/MobilenetV2/expanded_conv_1/',\n",
      "          'FeatureExtractor/MobilenetV2/expanded_conv_2/',\n",
      "          'FeatureExtractor/MobilenetV2/expanded_conv_3/',\n",
      "          'FeatureExtractor/MobilenetV2/expanded_conv_4/',\n",
      "          'FeatureExtractor/MobilenetV2/expanded_conv_5/',\n",
      "          'FeatureExtractor/MobilenetV2/expanded_conv_6/',\n",
      "          'FeatureExtractor/MobilenetV2/expanded_conv_7/']\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31mtrain_input_reader {\n",
      "  label_map_path: \"/opt/ml/input/data/training/map.pbtxt\"\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"/opt/ml/input/data/training/train.record\"\n",
      "  }\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31meval_config {\n",
      "  num_examples: 8000\n",
      "  metrics_set: \"coco_detection_metrics\"\n",
      "  use_moving_averages: false\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31meval_input_reader {\n",
      "  label_map_path: \"/opt/ml/input/data/training/map.pbtxt\"\n",
      "  shuffle: false\n",
      "  num_readers: 1\n",
      "  tf_record_input_reader {\n",
      "    input_path: \"/opt/ml/input/data/training/eval.record\"\n",
      "  }\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31mgraph_rewriter {\n",
      "  quantization {\n",
      "    delay: 48000\n",
      "    weight_bits: 8\n",
      "    activation_bits: 8\n",
      "  }\u001b[0m\n",
      "\u001b[31m}\u001b[0m\n",
      "\u001b[31m/tensorflow/models/research\u001b[0m\n",
      "\u001b[31mmodel.ckpt.data-00000-of-00001\u001b[0m\n",
      "\u001b[31mmodel.ckpt.index\u001b[0m\n",
      "\u001b[31mmodel.ckpt.meta\u001b[0m\n",
      "\u001b[31mpipeline.config\u001b[0m\n",
      "\u001b[31mtflite_graph.pb\u001b[0m\n",
      "\u001b[31mtflite_graph.pbtxt\u001b[0m\n",
      "\u001b[31m+ cp pipeline.config /tensorflow/models/research/learn/ckpt/pipeline.config\u001b[0m\n",
      "\u001b[31m++ python3 hyper.py\u001b[0m\n",
      "\u001b[31m+ TRAIN_STEPS=2000\u001b[0m\n",
      "\u001b[31m+ echo 'Beginning training on Docker image'\u001b[0m\n",
      "\u001b[31m+ ./retrain_detection_model.sh --num_training_steps 2000 --num_eval_steps 1\u001b[0m\n",
      "\u001b[31mBeginning training on Docker image\u001b[0m\n",
      "\u001b[31m+ num_training_steps=500\u001b[0m\n",
      "\u001b[31m+ [[ 4 -gt 0 ]]\u001b[0m\n",
      "\u001b[31m+ case \"$1\" in\u001b[0m\n",
      "\u001b[31m+ num_training_steps=2000\u001b[0m\n",
      "\u001b[31m+ shift 2\u001b[0m\n",
      "\u001b[31m+ [[ 2 -gt 0 ]]\u001b[0m\n",
      "\u001b[31m+ case \"$1\" in\u001b[0m\n",
      "\u001b[31m+ num_eval_steps=1\u001b[0m\n",
      "\u001b[31m+ shift 2\u001b[0m\n",
      "\u001b[31m+ [[ 0 -gt 0 ]]\u001b[0m\n",
      "\u001b[31m+ source /tensorflow/models/research/constants.sh\u001b[0m\n",
      "\u001b[31m++ declare -A ckpt_link_map\u001b[0m\n",
      "\u001b[31m++ declare -A ckpt_name_map\u001b[0m\n",
      "\u001b[31m++ declare -A config_filename_map\u001b[0m\n",
      "\u001b[31m++ ckpt_link_map[\"mobilenet_v1_ssd\"]=http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18.tar.gz\u001b[0m\n",
      "\u001b[31m++ ckpt_link_map[\"mobilenet_v2_ssd\"]=http://download.tensorflow.org/models/object_detection/ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03.tar.gz\u001b[0m\n",
      "\u001b[31m++ ckpt_name_map[\"mobilenet_v1_ssd\"]=ssd_mobilenet_v1_quantized_300x300_coco14_sync_2018_07_18\u001b[0m\n",
      "\u001b[31m++ ckpt_name_map[\"mobilenet_v2_ssd\"]=ssd_mobilenet_v2_quantized_300x300_coco_2019_01_03\u001b[0m\n",
      "\u001b[31m++ config_filename_map[\"mobilenet_v1_ssd-true\"]=pipeline_mobilenet_v1_ssd_retrain_whole_model.config\u001b[0m\n",
      "\u001b[31m++ config_filename_map[\"mobilenet_v1_ssd-false\"]=pipeline_mobilenet_v1_ssd_retrain_last_few_layers.config\u001b[0m\n",
      "\u001b[31m++ config_filename_map[\"mobilenet_v2_ssd-true\"]=pipeline_mobilenet_v2_ssd_retrain_whole_model.config\u001b[0m\n",
      "\u001b[31m++ config_filename_map[\"mobilenet_v2_ssd-false\"]=pipeline_mobilenet_v2_ssd_retrain_last_few_layers.config\u001b[0m\n",
      "\u001b[31m++ INPUT_TENSORS=normalized_input_image_tensor\u001b[0m\n",
      "\u001b[31m++ OUTPUT_TENSORS=TFLite_Detection_PostProcess,TFLite_Detection_PostProcess:1,TFLite_Detection_PostProcess:2,TFLite_Detection_PostProcess:3\u001b[0m\n",
      "\u001b[31m++ OBJ_DET_DIR=/tensorflow/models/research\u001b[0m\n",
      "\u001b[31m++ LEARN_DIR=/tensorflow/models/research/learn\u001b[0m\n",
      "\u001b[31m++ CKPT_DIR=/tensorflow/models/research/learn/ckpt\u001b[0m\n",
      "\u001b[31m++ TRAIN_DIR=/tensorflow/models/research/learn/train\u001b[0m\n",
      "\u001b[31m++ OUTPUT_DIR=/tensorflow/models/research/learn/models\u001b[0m\n",
      "\u001b[31m+ mkdir /tensorflow/models/research/learn/train\u001b[0m\n",
      "\u001b[31m+ python object_detection/model_main.py --pipeline_config_path=/tensorflow/models/research/learn/ckpt/pipeline.config --model_dir=/tensorflow/models/research/learn/train --num_train_steps=2000 --num_eval_steps=1\u001b[0m\n",
      "\u001b[31m/tensorflow/models/research/object_detection/utils/visualization_utils.py:29: UserWarning: \u001b[0m\n",
      "\u001b[31mThis call to matplotlib.use() has no effect because the backend has already\u001b[0m\n",
      "\u001b[31mbeen chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\u001b[0m\n",
      "\u001b[31mor matplotlib.backends is imported for the first time.\n",
      "\u001b[0m\n",
      "\u001b[31mThe backend was *originally* set to 'TkAgg' by the following code:\n",
      "  File \"object_detection/model_main.py\", line 26, in <module>\n",
      "    from object_detection import model_lib\n",
      "  File \"/tensorflow/models/research/object_detection/model_lib.py\", line 27, in <module>\n",
      "    from object_detection import eval_util\n",
      "  File \"/tensorflow/models/research/object_detection/eval_util.py\", line 33, in <module>\n",
      "    from object_detection.metrics import coco_evaluation\n",
      "  File \"/tensorflow/models/research/object_detection/metrics/coco_evaluation.py\", line 25, in <module>\n",
      "    from object_detection.metrics import coco_tools\n",
      "  File \"/tensorflow/models/research/object_detection/metrics/coco_tools.py\", line 51, in <module>\n",
      "    from pycocotools import coco\n",
      "  File \"/tensorflow/models/research/pycocotools/coco.py\", line 49, in <module>\n",
      "    import matplotlib.pyplot as plt\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/pyplot.py\", line 71, in <module>\n",
      "    from matplotlib.backends import pylab_setup\n",
      "  File \"/usr/local/lib/python2.7/dist-packages/matplotlib/backends/__init__.py\", line 16, in <module>\n",
      "    line for line in traceback.format_stack()\n",
      "\n",
      "\n",
      "  import matplotlib; matplotlib.use('Agg')  # pylint: disable=multiple-statements\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:Forced number of epochs for all eval validations to be 1.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:Expected number of evaluation epochs is 1, but instead encountered `eval_on_train_input_config.num_epochs` = 0. Overwriting `num_epochs` to 1.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:Estimator's model_fn (<function model_fn at 0x7f6993a5d578>) includes params argument, but params are not passed to Estimator.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:num_readers has been reduced to 1 to match input file shards.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /tensorflow/models/research/object_detection/builders/dataset_builder.py:86: parallel_interleave (from tensorflow.contrib.data.python.ops.interleave_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mUse `tf.data.experimental.parallel_interleave(...)`.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/python/ops/sparse_ops.py:1165: sparse_to_dense (from tensorflow.python.ops.sparse_ops) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mCreate a `tf.sparse.SparseTensor` and use `tf.sparse.to_dense` instead.\u001b[0m\n",
      "\u001b[31mWARNING:tensorflow:From /tensorflow/models/research/object_detection/builders/dataset_builder.py:158: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[31mInstructions for updating:\u001b[0m\n",
      "\u001b[31mUse `tf.data.Dataset.batch(..., drop_remainder=True)`.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_0/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[273]], model variable shape: [[24]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_0/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 576, 273]], model variable shape: [[1, 1, 576, 24]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_1/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_1/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 1280, 546]], model variable shape: [[1, 1, 1280, 48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_2/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_2/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 512, 546]], model variable shape: [[1, 1, 512, 48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_3/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_3/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_4/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_4/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 256, 546]], model variable shape: [[1, 1, 256, 48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_5/ClassPredictor/biases] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[546]], model variable shape: [[48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [BoxPredictor_5/ClassPredictor/weights] is available in checkpoint, but has an incompatible shape with model variable. Checkpoint shape: [[1, 1, 128, 546]], model variable shape: [[1, 1, 128, 48]]. This variable will not be initialized from the checkpoint.\u001b[0m\n",
      "\u001b[31mWARNING:root:Variable [global_step] is not available in checkpoint\u001b[0m\n",
      "\u001b[31m2019-09-19 00:26:33.261771: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mRunning per image evaluation...\u001b[0m\n",
      "\u001b[31mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[31mDONE (t=1.27s).\u001b[0m\n",
      "\u001b[31mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[31mDONE (t=0.37s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.072\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.180\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.045\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.017\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.074\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.212\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.085\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.174\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.179\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.055\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.176\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.333\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mRunning per image evaluation...\u001b[0m\n",
      "\u001b[31mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[31mDONE (t=1.05s).\u001b[0m\n",
      "\u001b[31mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[31mDONE (t=0.33s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.102\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.240\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.074\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.017\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.072\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.298\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.100\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.175\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.176\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.042\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.145\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.401\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mRunning per image evaluation...\u001b[0m\n",
      "\u001b[31mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[31mDONE (t=0.98s).\u001b[0m\n",
      "\u001b[31mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[31mDONE (t=0.33s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.118\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.247\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.103\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.032\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.104\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.320\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.109\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.188\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.189\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.073\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.164\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.437\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mRunning per image evaluation...\u001b[0m\n",
      "\u001b[31mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[31mDONE (t=0.99s).\u001b[0m\n",
      "\u001b[31mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[31mDONE (t=0.32s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.131\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.269\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.109\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.119\u001b[0m\n",
      "\u001b[31m Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.367\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.133\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.222\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.223\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.085\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.195\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.478\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mRunning per image evaluation...\u001b[0m\n",
      "\u001b[31mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[31mDONE (t=0.96s).\u001b[0m\n",
      "\u001b[31mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[31mDONE (t=0.31s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.145\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.310\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.115\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.046\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.130\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.370\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.142\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.249\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.249\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.096\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.202\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.548\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mRunning per image evaluation...\u001b[0m\n",
      "\u001b[31mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[31mDONE (t=1.03s).\u001b[0m\n",
      "\u001b[31mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[31mDONE (t=0.32s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.164\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.341\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.128\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.049\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.136\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.422\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.156\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.269\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.269\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.117\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.224\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.565\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mRunning per image evaluation...\u001b[0m\n",
      "\u001b[31mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[31mDONE (t=0.95s).\u001b[0m\n",
      "\u001b[31mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[31mDONE (t=0.32s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.151\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.324\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.114\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.044\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.135\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.405\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.148\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.248\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.249\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.094\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.199\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.556\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mRunning per image evaluation...\u001b[0m\n",
      "\u001b[31mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[31mDONE (t=0.97s).\u001b[0m\n",
      "\u001b[31mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[31mDONE (t=0.32s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.164\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.331\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.122\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.041\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.141\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.440\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.155\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.265\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.266\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.105\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.582\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mRunning per image evaluation...\u001b[0m\n",
      "\u001b[31mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[31mDONE (t=0.97s).\u001b[0m\n",
      "\u001b[31mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[31mDONE (t=0.31s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.157\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.335\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.117\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.042\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.143\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.419\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.163\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.261\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.264\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.103\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.223\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.582\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mRunning per image evaluation...\u001b[0m\n",
      "\u001b[31mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[31mDONE (t=0.95s).\u001b[0m\n",
      "\u001b[31mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[31mDONE (t=0.33s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.158\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.329\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.121\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.040\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.137\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.420\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.149\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.259\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.259\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.103\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.214\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.574\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mcreating index...\u001b[0m\n",
      "\u001b[31mindex created!\u001b[0m\n",
      "\u001b[31mRunning per image evaluation...\u001b[0m\n",
      "\u001b[31mEvaluate annotation type *bbox*\u001b[0m\n",
      "\u001b[31mDONE (t=0.96s).\u001b[0m\n",
      "\u001b[31mAccumulating evaluation results...\u001b[0m\n",
      "\u001b[31mDONE (t=0.31s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.154\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.330\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.108\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.039\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.132\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.409\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# The type of computer used for training. The default is recommended. It costs 45 cents/hour to run.\n",
    "instance_type = 'ml.c4.2xlarge'\n",
    "\n",
    "# The number of epochs to train to. 500 is a safe number. With the default instance, it should take 45 minutes.\n",
    "hyperparameters = {'epochs': 2000}\n",
    "\n",
    "# The estimator object, using our notebook, training instance, the ECR image, and the specified training steps\n",
    "estimator = Estimator(role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=instance_type,\n",
    "                      image_name=ecr_image,\n",
    "                      hyperparameters=hyperparameters)\n",
    "\n",
    "# Change this bucket if you want to train with your own data. The WPILib bucket contains thousands of high quality labeled images.\n",
    "# s3://wpilib\n",
    "estimator.fit(\"s3://wpilib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The output\n",
    "\n",
    "Go to the Training Jobs tab of SageMaker. Click on the newest Completed job. Scroll to the bottom. The S3 bucket containing the trained .tflite file (inside of a tar file) can be found there.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
