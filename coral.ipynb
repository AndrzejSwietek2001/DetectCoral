{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WPILib ML Notebook\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "By using this notebook, you can train a TensorFlow Lite model for use on a Raspberry Pi and Google Coral USB Accelerator. We've designed this process to be as simple as possible. If you find an issue with this notebook, please create a new issue report on our GitHub page, where you downloaded this notebook.\n",
    "\n",
    "## Workflow\n",
    "\n",
    "This section will explain the four distinct steps to getting a trained model to run on your hardware.\n",
    "### Getting Data\n",
    "\n",
    "WPILib provides thousands of labelled images for this years game, which you can download here. However, you can train with custom data using this notebook as well. The below instructions describe how to gather and label your own data.\n",
    "\n",
    "1. Plug a USB Camera into your laptop, and run a script similar to record_video.py, which simply makes an mp4 from the camera stream.\n",
    "2. Create a [supervise.ly](supervise.ly) account. This is a very nice tool for labelling data.\n",
    "3. (Optional) You can add other teammates to your Supervise.ly workspace by clicking 'Members' on the left and then 'INVITE' at the top.\n",
    "4. Choose a workspace to work in, in the 'Workspaces' tab.\n",
    "5. Upload the official WPILib labelled data to your workspace. Download the tar here, extract it, then click 'IMPORT DATA' or 'UPLOAD' inside of your workspace. Change the import plugin to Supervisely, then drag in the extracted FOLDER. Then, give the project a name, then click import.\n",
    "6. Upload your own video to your workspace. Click 'UPLOAD' when inside of your workspace, change your import plugin to video, drag in your video, give the project a name, and click import.\n",
    "7. Click into your newly import Dataset. Use the rectangle tool to draw appropriate boxes around the objects which you wish to label.\n",
    "\n",
    "### Training\n",
    "\n",
    "1. Download your datasets from Supervise.ly. Select the \"json and jpeg\" option.\n",
    "2. Upload your tar to a new folder in an Amazon S3 bucket, or a brand new S3 bucket.\n",
    "3. Create a new SageMaker notebook instance, and open the WPILib notebook.\n",
    "4. Change estimator.fit() to use your new dataset, by specifying the folder in which the tar is stored.\n",
    "5. Run each block of the notebook in order.\n",
    "6. Training should take roughly 45 minutes. If you do not change anything in the notebook, it should absolutely not take longer than an hour. In the Training Job, CPU usage should be around 690 during the majority of the training, if running on an ml.c4.2xlarge. If it is less, something went wrong.\n",
    "\n",
    "### Inference\n",
    "\n",
    "1. Go to the training job in SageMaker, scroll to the bottom, and find the output S3 location\n",
    "2. Download the the tar file in the bucket, extract it, and get your .tflite file\n",
    "3. Put the tflite on your Raspberry Pi by plugging in the SD card into your computer and dragging it in to /home/pi\n",
    "4. Run the python script, using `python3 object_detection.py --model output.tflite`\n",
    "\n",
    "\n",
    "## Notebook\n",
    "### Building and registering the container\n",
    "\n",
    "This code block runs a script that builds a docker container, and saves it as an Amazon ECR image. This image is used by the training instance so that all proper dependencies and WPILib files are in place.\n",
    "\n",
    "Just run this one, don't change anything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "#!/usr/bin/env bash\n",
    "docker system prune --force > /dev/null 2>&1\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=sagemaker-tf-wpi\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod -R +x coral/\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email) 2> /dev/null\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} . --no-cache > /dev/null 2>&1\n",
    "docker tag ${algorithm_name} ${fullname} > /dev/null 2>&1\n",
    "\n",
    "docker push ${fullname} > /dev/null 2>&1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get execution role\n",
    "\n",
    "This gets the notebook instance's execution role, used for communicating with the training instance.\n",
    "\n",
    "Just run this one too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training on SageMaker\n",
    "Training a model on SageMaker with the Python SDK is done in a way that is similar to the way we trained it locally. This is done by changing our train_instance_type from `local` to one of our [supported EC2 instance types](https://aws.amazon.com/sagemaker/pricing/instance-types/).\n",
    "\n",
    "In addition, we must now specify the ECR image URL, which we just pushed above.\n",
    "\n",
    "Just run this one too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name\n",
    "\n",
    "\n",
    "# If you want to change the algorithm name, make sure to change the name in the first block too.\n",
    "algorithm_name = 'sagemaker-tf-wpi'\n",
    "\n",
    "ecr_image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)\n",
    "\n",
    "print(ecr_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This last step runs the training instance (default an ml.c4.2xlarge), and begins training with the data specified in `fit()`\n",
    "\n",
    "This section has lots of values worth changing\n",
    "`instance_type`: change the type of computer that the training will run on.\n",
    "`hyperparameters`: change the number of epochs.\n",
    "`estimator.fit(...)`: change the location of the data used for training. should be in the format `\"s3://BUCKET-NAME\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# The type of computer used for training. The default is recommended. It costs 45 cents/hour to run.\n",
    "instance_type = 'ml.c4.2xlarge'\n",
    "\n",
    "# The number of epochs to train to. 500 is a safe number. With the default instance, it should take 45 minutes.\n",
    "hyperparameters = {'epochs': 2000}\n",
    "\n",
    "# The estimator object, using our notebook, training instance, the ECR image, and the specified training steps\n",
    "estimator = Estimator(role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=instance_type,\n",
    "                      image_name=ecr_image,\n",
    "                      hyperparameters=hyperparameters)\n",
    "\n",
    "# Change this bucket if you want to train with your own data. The WPILib bucket contains thousands of high quality labeled images.\n",
    "# s3://wpilib\n",
    "estimator.fit(\"s3://wpilib\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# The output\n",
    "\n",
    "Go to the Training Jobs tab of SageMaker. Click on the newest Completed job. Scroll to the bottom. The S3 bucket containing the trained .tflite file (inside of a tar file) can be found there.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
