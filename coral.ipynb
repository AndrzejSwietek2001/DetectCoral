{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WPILib ML Notebook\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "By using this notebook, you can train a TensorFlow Lite model for use on a Raspberry Pi and Google Coral USB Accelerator. We've designed this process to be as simple as possible. If you find an issue with this notebook, please create a new issue report on our [GitHub page](https://github.com/wpilibsuite/CoralSagemaker), where you downloaded this notebook.\n",
    "\n",
    "### Training\n",
    "\n",
    "1. Download the WPILIB dataset as a .tar file [here](https://github.com/wpilibsuite/CoralSagemaker/releases/download/v1/WPILib.tar)\n",
    "2. Upload your .tar file to a new folder in an Amazon S3 bucket, or a brand new S3 bucket.\n",
    "3. Create a new SageMaker notebook instance, and open the WPILib notebook.\n",
    "4. Change estimator.fit() in the last code cell to use your new dataset, by specifying the folder in which the tar is stored.\n",
    "5. Run the code block.\n",
    "6. Training should take roughly 10 minutes and cost roughly \\\\$0.55 if using the GPU instance, or 45 minutes and cost roughly \\\\$0.45 if using the CPU instance. If you do not change anything in the notebook, other than the S3 location, it should absolutely not take longer than an hour.\n",
    "\n",
    "## Notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This step runs the training instance (default for GPU is a ml.p3.2xlarge and for the default is CPU is an ml.c4.2xlarge), and begins training with the data specified in `fit()`\n",
    "\n",
    "This section has lots of configurable values\n",
    "You need to change `estimator.fit(...)`:to be the location of the data used for training. (the bucket you uploaded the .tar to) It should be in the format `\"s3://BUCKET-NAME\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-12-17 23:02:31 Starting - Starting the training job...\n",
      "2019-12-17 23:02:32 Starting - Launching requested ML instances......\n",
      "2019-12-17 23:03:38 Starting - Preparing the instances for training...\n",
      "2019-12-17 23:04:28 Downloading - Downloading input data...\n",
      "2019-12-17 23:04:39 Training - Downloading the training image...........\u001b[34mDownloading model\u001b[0m\n",
      "\n",
      "2019-12-17 23:06:43 Training - Training image download completed. Training in progress.\u001b[34mSuccessfully created the TFRecords: /opt/ml/input/data/training/train.record\u001b[0m\n",
      "\u001b[34mSuccessfully created the TFRecords: /opt/ml/input/data/training/eval.record\u001b[0m\n",
      "\u001b[34mRecords generated.\u001b[0m\n",
      "\u001b[34mBeginning training on Docker image\u001b[0m\n",
      "\u001b[34mConverting checkpoint to tflite\u001b[0m\n",
      "\u001b[34mCompiling model for Edge TPU\u001b[0m\n",
      "\n",
      "2019-12-17 23:13:22 Uploading - Uploading generated training model\n",
      "2019-12-17 23:13:22 Completed - Training job completed\n",
      "Training seconds: 534\n",
      "Billable seconds: 534\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "# Uses GPU by default, change to false to use CPU\n",
    "use_gpu = True\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "instance_type = None\n",
    "algorithm_name = None\n",
    "\n",
    "if not use_gpu:\n",
    "    instance_type = 'ml.c4.2xlarge'\n",
    "    algorithm_name = 'sagemaker-tf-wpi'\n",
    "else:\n",
    "    instance_type = 'ml.p3.2xlarge'\n",
    "    algorithm_name = 'wpi-gpu'\n",
    "\n",
    "# The number of epochs to train to. 1000 is a safe number. With the default instance, it should take 45 minutes.\n",
    "# Batch size is the number of images in a round of training. 32 is a safe bet with the default GPU instance.\n",
    "hyperparameters = {'epochs': 1000,\n",
    "                  'batch_size': 32}\n",
    "\n",
    "ecr_image = \"249838237784.dkr.ecr.us-east-1.amazonaws.com/{}:latest\".format(algorithm_name)\n",
    "\n",
    "# The estimator object, using our notebook, training instance, the ECR image, and the specified training steps\n",
    "estimator = Estimator(role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=instance_type,\n",
    "                      image_name=ecr_image,\n",
    "                      hyperparameters=hyperparameters)\n",
    "\n",
    "# Change this bucket if you want to train with your own data. The WPILib bucket contains thousands of high quality labeled images.\n",
    "# s3://wpilib\n",
    "estimator.fit(\"s3://wpilib\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
