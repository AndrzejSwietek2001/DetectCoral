{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WPILib ML Notebook\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "By using this notebook, you can train a TensorFlow Lite model for use on a Raspberry Pi and Google Coral USB Accelerator. We've designed this process to be as simple as possible. If you find an issue with this notebook, please create a new issue report on our [GitHub page](https://github.com/GrantPerkins/CoralSagemaker), where you downloaded this notebook.\n",
    "\n",
    "### Training\n",
    "\n",
    "1. Download the WPILIB dataset as a .tar file [here](https://github.com/GrantPerkins/CoralSagemaker/releases/download/v1/WPILib.tar)\n",
    "2. Upload your .tar file to a new folder in an Amazon S3 bucket, or a brand new S3 bucket.\n",
    "3. Create a new SageMaker notebook instance, and open the WPILib notebook.\n",
    "4. Change estimator.fit() in the last code cell to use your new dataset, by specifying the folder in which the tar is stored.\n",
    "5. Run the code block.\n",
    "6. Training should take roughly 10 minutes and cost roughly \\\\$0.55 if using the GPU instance, or 45 minutes and cost roughly \\\\$0.45 if using the CPU instance. If you do not change anything in the notebook, other than the S3 location, it should absolutely not take longer than an hour.\n",
    "\n",
    "## Notebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "Sending build context to Docker daemon  124.9kB\n",
      "Step 1/18 : FROM tensorflow/tensorflow:1.12.0-rc2-devel-gpu\n",
      "1.12.0-rc2-devel-gpu: Pulling from tensorflow/tensorflow\n",
      "18d680d61657: Pulling fs layer\n",
      "0addb6fece63: Pulling fs layer\n",
      "78e58219b215: Pulling fs layer\n",
      "eb6959a66df2: Pulling fs layer\n",
      "f7221b275362: Pulling fs layer\n",
      "c7ca04507ec7: Pulling fs layer\n",
      "d9e715e7068d: Pulling fs layer\n",
      "baf5b702c331: Pulling fs layer\n",
      "eb330861cdf2: Pulling fs layer\n",
      "2441121cf9c3: Pulling fs layer\n",
      "bb13cab9e8e9: Pulling fs layer\n",
      "b5457b255e98: Pulling fs layer\n",
      "39628178caf5: Pulling fs layer\n",
      "6f161e5b9ca4: Pulling fs layer\n",
      "861c0316324d: Pulling fs layer\n",
      "9fa8bcd890df: Pulling fs layer\n",
      "df5a9509bf55: Pulling fs layer\n",
      "bab3ba4084cc: Pulling fs layer\n",
      "1877b28fd83d: Pulling fs layer\n",
      "1068d290c01c: Pulling fs layer\n",
      "eb6959a66df2: Waiting\n",
      "f7221b275362: Waiting\n",
      "c7ca04507ec7: Waiting\n",
      "d9e715e7068d: Waiting\n",
      "baf5b702c331: Waiting\n",
      "eb330861cdf2: Waiting\n",
      "2441121cf9c3: Waiting\n",
      "b5457b255e98: Waiting\n",
      "bb13cab9e8e9: Waiting\n",
      "39628178caf5: Waiting\n",
      "df5a9509bf55: Waiting\n",
      "6f161e5b9ca4: Waiting\n",
      "861c0316324d: Waiting\n",
      "9fa8bcd890df: Waiting\n",
      "bab3ba4084cc: Waiting\n",
      "1877b28fd83d: Waiting\n",
      "1068d290c01c: Waiting\n",
      "78e58219b215: Verifying Checksum\n",
      "78e58219b215: Download complete\n",
      "0addb6fece63: Verifying Checksum\n",
      "0addb6fece63: Download complete\n",
      "eb6959a66df2: Verifying Checksum\n",
      "eb6959a66df2: Download complete\n",
      "f7221b275362: Verifying Checksum\n",
      "f7221b275362: Download complete\n",
      "18d680d61657: Verifying Checksum\n",
      "18d680d61657: Download complete\n",
      "c7ca04507ec7: Download complete\n",
      "d9e715e7068d: Verifying Checksum\n",
      "d9e715e7068d: Download complete\n",
      "2441121cf9c3: Verifying Checksum\n",
      "2441121cf9c3: Download complete\n",
      "bb13cab9e8e9: Verifying Checksum\n",
      "bb13cab9e8e9: Download complete\n",
      "b5457b255e98: Download complete\n",
      "39628178caf5: Verifying Checksum\n",
      "39628178caf5: Download complete\n",
      "6f161e5b9ca4: Verifying Checksum\n",
      "6f161e5b9ca4: Download complete\n",
      "861c0316324d: Verifying Checksum\n",
      "861c0316324d: Download complete\n",
      "9fa8bcd890df: Verifying Checksum\n",
      "9fa8bcd890df: Download complete\n",
      "18d680d61657: Pull complete\n",
      "eb330861cdf2: Verifying Checksum\n",
      "eb330861cdf2: Download complete\n",
      "bab3ba4084cc: Verifying Checksum\n",
      "bab3ba4084cc: Download complete\n",
      "df5a9509bf55: Verifying Checksum\n",
      "df5a9509bf55: Download complete\n",
      "1877b28fd83d: Verifying Checksum\n",
      "1877b28fd83d: Download complete\n",
      "0addb6fece63: Pull complete\n",
      "78e58219b215: Pull complete\n",
      "1068d290c01c: Verifying Checksum\n",
      "1068d290c01c: Download complete\n",
      "baf5b702c331: Verifying Checksum\n",
      "baf5b702c331: Download complete\n",
      "eb6959a66df2: Pull complete\n",
      "f7221b275362: Pull complete\n",
      "c7ca04507ec7: Pull complete\n",
      "d9e715e7068d: Pull complete\n",
      "baf5b702c331: Pull complete\n",
      "eb330861cdf2: Pull complete\n",
      "2441121cf9c3: Pull complete\n",
      "bb13cab9e8e9: Pull complete\n",
      "b5457b255e98: Pull complete\n",
      "39628178caf5: Pull complete\n",
      "6f161e5b9ca4: Pull complete\n",
      "861c0316324d: Pull complete\n",
      "9fa8bcd890df: Pull complete\n",
      "df5a9509bf55: Pull complete\n",
      "bab3ba4084cc: Pull complete\n",
      "1877b28fd83d: Pull complete\n",
      "1068d290c01c: Pull complete\n",
      "Digest: sha256:68f20b1143eeeea49c5a834323f9609019c18120d63e362e0f6d042ae821f745\n",
      "Status: Downloaded newer image for tensorflow/tensorflow:1.12.0-rc2-devel-gpu\n",
      " ---> ad9d164e338c\n",
      "Step 2/18 : RUN git clone https://github.com/tensorflow/models.git &&     mv models /tensorflow/models\n",
      " ---> Running in 49985284f740\n",
      "\u001b[91mCloning into 'models'...\n",
      "\u001b[0m\u001b[91mChecking out files: 100% (3194/3194), done.\n",
      "\u001b[0mRemoving intermediate container 49985284f740\n",
      " ---> 6278920f1ced\n",
      "Step 3/18 : RUN apt-get update -qq > /dev/null &&     apt-get install -y python python-tk python3 python3-pip -qq > /dev/null\n",
      " ---> Running in ffcdacd31e9e\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mRemoving intermediate container ffcdacd31e9e\n",
      " ---> 4f659786d38d\n",
      "Step 4/18 : RUN apt-get update -qq > /dev/null &&     apt-get install -y --no-install-recommends nginx curl -qq > /dev/null\n",
      " ---> Running in c724d92a8bad\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mRemoving intermediate container c724d92a8bad\n",
      " ---> a7b25d43af60\n",
      "Step 5/18 : RUN pip install Cython contextlib2  pillow lxml jupyter matplotlib pandas -q &&     python3 -m pip install pandas -q\n",
      " ---> Running in 711b6bb13633\n",
      "\u001b[91mYou are using pip version 18.1, however version 19.3.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "\u001b[0mRemoving intermediate container 711b6bb13633\n",
      " ---> 8cb8c6d2d9bf\n",
      "Step 6/18 : RUN curl -s -OL \"https://github.com/google/protobuf/releases/download/v3.0.0/protoc-3.0.0-linux-x86_64.zip\" > /dev/null &&     unzip protoc-3.0.0-linux-x86_64.zip -d proto3 > /dev/null &&     mv proto3/bin/* /usr/local/bin &&     mv proto3/include/* /usr/local/include &&     rm -rf proto3 protoc-3.0.0-linux-x86_64.zip\n",
      " ---> Running in d0b9904ecc4c\n",
      "Removing intermediate container d0b9904ecc4c\n",
      " ---> ef5c8a45dfab\n",
      "Step 7/18 : RUN git clone --depth 1 https://github.com/cocodataset/cocoapi.git &&     cd cocoapi/PythonAPI &&     make -j8 &&     cp -r pycocotools /tensorflow/models/research &&     cd ../../ &&     rm -rf cocoapi\n",
      " ---> Running in d9e938d88eb0\n",
      "\u001b[91mCloning into 'cocoapi'...\n",
      "\u001b[0mpython setup.py build_ext --inplace\n",
      "running build_ext\n",
      "cythoning pycocotools/_mask.pyx to pycocotools/_mask.c\n",
      "\u001b[91m/usr/local/lib/python2.7/dist-packages/Cython/Compiler/Main.py:369: FutureWarning: Cython directive 'language_level' not set, using 2 for now (Py2). This will change in a later release! File: /root/cocoapi/PythonAPI/pycocotools/_mask.pyx\n",
      "  tree = Parsing.p_module(s, pxd, full_module_name)\n",
      "\u001b[0mbuilding 'pycocotools._mask' extension\n",
      "creating build\n",
      "creating build/common\n",
      "creating build/temp.linux-x86_64-2.7\n",
      "creating build/temp.linux-x86_64-2.7/pycocotools\n",
      "x86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/local/lib/python2.7/dist-packages/numpy/core/include -I../common -I/usr/include/python2.7 -c ../common/maskApi.c -o build/temp.linux-x86_64-2.7/../common/maskApi.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "\u001b[91m../common/maskApi.c: In function 'rleToBbox':\n",
      "\u001b[0m\u001b[91m../common/maskApi.c:141:31: warning: 'xp' may be used uninitialized in this function [-Wmaybe-uninitialized]\n",
      "       if(j%2==0) xp=x; else if(xp<x) { ys=0; ye=h-1; }\n",
      "                               ^\n",
      "\u001b[0mx86_64-linux-gnu-gcc -pthread -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -fno-strict-aliasing -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -fPIC -I/usr/local/lib/python2.7/dist-packages/numpy/core/include -I../common -I/usr/include/python2.7 -c pycocotools/_mask.c -o build/temp.linux-x86_64-2.7/pycocotools/_mask.o -Wno-cpp -Wno-unused-function -std=c99\n",
      "creating build/lib.linux-x86_64-2.7\n",
      "creating build/lib.linux-x86_64-2.7/pycocotools\n",
      "x86_64-linux-gnu-gcc -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -fno-strict-aliasing -DNDEBUG -g -fwrapv -O2 -Wall -Wstrict-prototypes -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security -Wl,-Bsymbolic-functions -Wl,-z,relro -Wdate-time -D_FORTIFY_SOURCE=2 -g -fstack-protector-strong -Wformat -Werror=format-security build/temp.linux-x86_64-2.7/../common/maskApi.o build/temp.linux-x86_64-2.7/pycocotools/_mask.o -o build/lib.linux-x86_64-2.7/pycocotools/_mask.so\n",
      "copying build/lib.linux-x86_64-2.7/pycocotools/_mask.so -> pycocotools\n",
      "rm -rf build\n",
      "Removing intermediate container d9e938d88eb0\n",
      " ---> 9bf432f4c194\n",
      "Step 8/18 : RUN cd /tensorflow/models/research &&     protoc object_detection/protos/*.proto --python_out=.\n",
      " ---> Running in 861e1d5f67b5\n",
      "Removing intermediate container 861e1d5f67b5\n",
      " ---> d4b2a1845a17\n",
      "Step 9/18 : ENV PYTHONPATH $PYTHONPATH:/tensorflow/models/research:/tensorflow/models/research/slim\n",
      " ---> Running in a01c8953f62e\n",
      "Removing intermediate container a01c8953f62e\n",
      " ---> a31c47862d6e\n",
      "Step 10/18 : RUN apt-get update -qq > /dev/null &&     apt-get install -y wget vim emacs nano -qq > /dev/null\n",
      " ---> Running in 56e31b52741e\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mRemoving intermediate container 56e31b52741e\n",
      " ---> 5e58b277dcac\n",
      "Step 11/18 : ARG work_dir=/tensorflow/models/research\n",
      " ---> Running in f4a794978cbe\n",
      "Removing intermediate container f4a794978cbe\n",
      " ---> 2b539df0f62b\n",
      "Step 12/18 : ARG scripts_link=\"http://storage.googleapis.com/cloud-iot-edge-pretrained-models/docker/obj_det_scripts.tgz\"\n",
      " ---> Running in 04eb2921e6dc\n",
      "Removing intermediate container 04eb2921e6dc\n",
      " ---> eb11aee84ac4\n",
      "Step 13/18 : RUN cd ${work_dir} &&     wget -q -O obj_det_scripts.tgz ${scripts_link} &&     tar zxf obj_det_scripts.tgz\n",
      " ---> Running in 3b6b09fce4df\n",
      "Removing intermediate container 3b6b09fce4df\n",
      " ---> 610c875e4fb2\n",
      "Step 14/18 : RUN apt-get install -y apt-transport-https ca-certificates\n",
      " ---> Running in 414a92933a1f\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following packages will be upgraded:\n",
      "  apt-transport-https ca-certificates\n",
      "2 upgraded, 0 newly installed, 0 to remove and 104 not upgraded.\n",
      "Need to get 194 kB of archives.\n",
      "After this operation, 1024 B of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 apt-transport-https amd64 1.2.32 [26.5 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 ca-certificates all 20170717~16.04.2 [167 kB]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 194 kB in 0s (292 kB/s)\n",
      "(Reading database ... 42117 files and directories currently installed.)\n",
      "Preparing to unpack .../apt-transport-https_1.2.32_amd64.deb ...\n",
      "Unpacking apt-transport-https (1.2.32) over (1.2.27) ...\n",
      "Preparing to unpack .../ca-certificates_20170717~16.04.2_all.deb ...\n",
      "Unpacking ca-certificates (20170717~16.04.2) over (20170717~16.04.1) ...\n",
      "Setting up apt-transport-https (1.2.32) ...\n",
      "Setting up ca-certificates (20170717~16.04.2) ...\n",
      "debconf: unable to initialize frontend: Dialog\n",
      "debconf: (TERM is not set, so the dialog frontend is not usable.)\n",
      "debconf: falling back to frontend: Readline\n",
      "Processing triggers for ca-certificates (20170717~16.04.2) ...\n",
      "Updating certificates in /etc/ssl/certs...\n",
      "0 added, 0 removed; done.\n",
      "Running hooks in /etc/ca-certificates/update.d...\n",
      "done.\n",
      "Removing intermediate container 414a92933a1f\n",
      " ---> 9183dcc28c56\n",
      "Step 15/18 : RUN curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | apt-key add - &&     echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | tee /etc/apt/sources.list.d/coral-edgetpu.list &&     apt-get update &&     apt-get install -y edgetpu\n",
      " ---> Running in fa37c42bf475\n",
      "\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91m%\u001b[0m\u001b[91m \u001b[0m\u001b[91mT\u001b[0m\u001b[91mo\u001b[0m\u001b[91mt\u001b[0m\u001b[91ma\u001b[0m\u001b[91ml\u001b[0m\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91m \u001b[0m\u001b[91m%\u001b[0m\u001b[91m Received % Xferd  Av\u001b[0m\u001b[91merage Speed   Time    T\u001b[0m\u001b[91mime     Time  \u001b[0m\u001b[91mCurrent\u001b[0m\u001b[91m\n",
      "          \u001b[0m\u001b[91m                       Dload  Upload\u001b[0m\u001b[91m   Total   Spe\u001b[0m\u001b[91mnt  \u001b[0m\u001b[91m  Left  Speed\n",
      "100   659  100   659    0     0   6058      0 --:--:-- --:--:-- --:--:--  6101[91m0      \u001b[0m\u001b[91m0 \u001b[0m\u001b[91m--:--:-- --:--:-- --:--:\u001b[0m\u001b[91m-\u001b[0m\u001b[91m-    \u001b[0m\u001b[91m 0\u001b[0m\u001b[91m\n",
      "\u001b[0mOK\n",
      "deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\n",
      "Get:1 file:/var/nvinfer-runtime-trt-repo-4.0.1-ga-cuda9.0  InRelease\n",
      "Ign:1 file:/var/nvinfer-runtime-trt-repo-4.0.1-ga-cuda9.0  InRelease\n",
      "Get:2 file:/var/nvinfer-runtime-trt-repo-4.0.1-ga-cuda9.0  Release [574 B]\n",
      "Get:2 file:/var/nvinfer-runtime-trt-repo-4.0.1-ga-cuda9.0  Release [574 B]\n",
      "Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  InRelease\n",
      "Hit:5 http://archive.ubuntu.com/ubuntu xenial InRelease\n",
      "Hit:6 http://security.ubuntu.com/ubuntu xenial-security InRelease\n",
      "Get:7 https://packages.cloud.google.com/apt coral-edgetpu-stable InRelease [6332 B]\n",
      "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  InRelease\n",
      "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1604/x86_64  Release\n",
      "Hit:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64  Release\n",
      "Hit:11 http://archive.ubuntu.com/ubuntu xenial-updates InRelease\n",
      "Hit:12 http://archive.ubuntu.com/ubuntu xenial-backports InRelease\n",
      "Get:13 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 Packages [1382 B]\n",
      "Fetched 7714 B in 0s (12.4 kB/s)\n",
      "Reading package lists...\n",
      "Reading package lists...\n",
      "Building dependency tree...\n",
      "Reading state information...\n",
      "The following additional packages will be installed:\n",
      "  edgetpu-compiler libedgetpu1-std\n",
      "The following NEW packages will be installed:\n",
      "  edgetpu edgetpu-compiler libedgetpu1-std\n",
      "0 upgraded, 3 newly installed, 0 to remove and 104 not upgraded.\n",
      "Need to get 4157 kB of archives.\n",
      "After this operation, 14.8 MB of additional disk space will be used.\n",
      "Get:1 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 edgetpu-compiler amd64 12-1 [3842 kB]\n",
      "Get:2 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 libedgetpu1-std amd64 12-1 [312 kB]\n",
      "Get:3 https://packages.cloud.google.com/apt coral-edgetpu-stable/main amd64 edgetpu all 12.1-1 [3322 B]\n",
      "\u001b[91mdebconf: delaying package configuration, since apt-utils is not installed\n",
      "\u001b[0mFetched 4157 kB in 0s (6988 kB/s)\n",
      "Selecting previously unselected package edgetpu-compiler.\n",
      "(Reading database ... 42117 files and directories currently installed.)\n",
      "Preparing to unpack .../edgetpu-compiler_12-1_amd64.deb ...\n",
      "Unpacking edgetpu-compiler (12-1) ...\n",
      "Selecting previously unselected package libedgetpu1-std:amd64.\n",
      "Preparing to unpack .../libedgetpu1-std_12-1_amd64.deb ...\n",
      "Unpacking libedgetpu1-std:amd64 (12-1) ...\n",
      "Selecting previously unselected package edgetpu.\n",
      "Preparing to unpack .../edgetpu_12.1-1_all.deb ...\n",
      "Unpacking edgetpu (12.1-1) ...\n",
      "Processing triggers for libc-bin (2.23-0ubuntu10) ...\n",
      "Setting up edgetpu-compiler (12-1) ...\n",
      "Setting up libedgetpu1-std:amd64 (12-1) ...\n",
      "Setting up edgetpu (12.1-1) ...\n",
      "Processing triggers for libc-bin (2.23-0ubuntu10) ...\n",
      "Removing intermediate container fa37c42bf475\n",
      " ---> c9396d9084f2\n",
      "Step 16/18 : COPY /coral /tensorflow/models/research\n",
      " ---> a502f16b8cd7\n",
      "Step 17/18 : ENV PATH $PATH:/tensorflow/models/research\n",
      " ---> Running in b33ef4016084\n",
      "Removing intermediate container b33ef4016084\n",
      " ---> ab79ca8bbef8\n",
      "Step 18/18 : WORKDIR ${work_dir}\n",
      " ---> Running in 3d6a7ac8c5fa\n",
      "Removing intermediate container 3d6a7ac8c5fa\n",
      " ---> 5405ebbc3d33\n",
      "Successfully built 5405ebbc3d33\n",
      "Successfully tagged wpi-gpu:latest\n",
      "The push refers to repository [249838237784.dkr.ecr.us-east-1.amazonaws.com/wpi-gpu]\n",
      "a66817bf9ee5: Preparing\n",
      "8f8e9bd3b1df: Preparing\n",
      "be19f7dcc640: Preparing\n",
      "4a5fe5e7557b: Preparing\n",
      "0babf404ea29: Preparing\n",
      "b24ca903321c: Preparing\n",
      "f7332d73bee7: Preparing\n",
      "bda3abedfea2: Preparing\n",
      "52d622d90ba4: Preparing\n",
      "addea679f0d7: Preparing\n",
      "0351398fee6b: Preparing\n",
      "0a11e8c0b2e5: Preparing\n",
      "004445fafe0c: Preparing\n",
      "e910b39af978: Preparing\n",
      "662ffa97f60b: Preparing\n",
      "fe6cf112aa56: Preparing\n",
      "178e6025c643: Preparing\n",
      "7c00b7168828: Preparing\n",
      "fcb015946add: Preparing\n",
      "f9ef3f340384: Preparing\n",
      "59f5fcae2f44: Preparing\n",
      "040b082885d2: Preparing\n",
      "dac02adbad6d: Preparing\n",
      "0cd8d564fc11: Preparing\n",
      "ef4942afda1b: Preparing\n",
      "25f1c70d2866: Preparing\n",
      "69d11cb505ea: Preparing\n",
      "c0cfd70d45d3: Preparing\n",
      "f1dfa8049aa6: Preparing\n",
      "79109c0f8a0b: Preparing\n",
      "33db8ccd260b: Preparing\n",
      "b8c891f0ffec: Preparing\n",
      "b24ca903321c: Waiting\n",
      "f7332d73bee7: Waiting\n",
      "bda3abedfea2: Waiting\n",
      "52d622d90ba4: Waiting\n",
      "addea679f0d7: Waiting\n",
      "0351398fee6b: Waiting\n",
      "0a11e8c0b2e5: Waiting\n",
      "004445fafe0c: Waiting\n",
      "e910b39af978: Waiting\n",
      "662ffa97f60b: Waiting\n",
      "fe6cf112aa56: Waiting\n",
      "178e6025c643: Waiting\n",
      "7c00b7168828: Waiting\n",
      "fcb015946add: Waiting\n",
      "f9ef3f340384: Waiting\n",
      "59f5fcae2f44: Waiting\n",
      "040b082885d2: Waiting\n",
      "dac02adbad6d: Waiting\n",
      "0cd8d564fc11: Waiting\n",
      "ef4942afda1b: Waiting\n",
      "b8c891f0ffec: Waiting\n",
      "25f1c70d2866: Waiting\n",
      "f1dfa8049aa6: Waiting\n",
      "79109c0f8a0b: Waiting\n",
      "69d11cb505ea: Waiting\n",
      "33db8ccd260b: Waiting\n",
      "c0cfd70d45d3: Waiting\n",
      "4a5fe5e7557b: Pushed\n",
      "a66817bf9ee5: Pushed\n",
      "be19f7dcc640: Pushed\n",
      "b24ca903321c: Pushed\n",
      "f7332d73bee7: Pushed\n",
      "8f8e9bd3b1df: Pushed\n",
      "bda3abedfea2: Pushed\n",
      "addea679f0d7: Pushed\n",
      "004445fafe0c: Layer already exists\n",
      "e910b39af978: Layer already exists\n",
      "662ffa97f60b: Layer already exists\n",
      "fe6cf112aa56: Layer already exists\n",
      "178e6025c643: Layer already exists\n",
      "7c00b7168828: Layer already exists\n",
      "fcb015946add: Layer already exists\n",
      "f9ef3f340384: Layer already exists\n",
      "59f5fcae2f44: Layer already exists\n",
      "040b082885d2: Layer already exists\n",
      "dac02adbad6d: Layer already exists\n",
      "0cd8d564fc11: Layer already exists\n",
      "ef4942afda1b: Layer already exists\n",
      "25f1c70d2866: Layer already exists\n",
      "69d11cb505ea: Layer already exists\n",
      "c0cfd70d45d3: Layer already exists\n",
      "f1dfa8049aa6: Layer already exists\n",
      "79109c0f8a0b: Layer already exists\n",
      "33db8ccd260b: Layer already exists\n",
      "b8c891f0ffec: Layer already exists\n",
      "0351398fee6b: Pushed\n",
      "52d622d90ba4: Pushed\n",
      "0babf404ea29: Pushed\n",
      "0a11e8c0b2e5: Pushed\n",
      "latest: digest: sha256:0b47cfd06b8085d21f49697c309e004adaf2a3a8a532570782fda62dd01277dc size: 7040\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "#!/usr/bin/env bash\n",
    "docker system prune --force > /dev/null 2>&1\n",
    "\n",
    "# The name of our algorithm\n",
    "algorithm_name=wpi-gpu\n",
    "\n",
    "cd container\n",
    "\n",
    "chmod -R +x coral/\n",
    "\n",
    "account=$(aws sts get-caller-identity --query Account --output text)\n",
    "\n",
    "# Get the region defined in the current configuration (default to us-west-2 if none defined)\n",
    "region=$(aws configure get region)\n",
    "region=${region:-us-west-2}\n",
    "\n",
    "fullname=\"${account}.dkr.ecr.${region}.amazonaws.com/${algorithm_name}:latest\"\n",
    "\n",
    "# If the repository doesn't exist in ECR, create it.\n",
    "\n",
    "aws ecr describe-repositories --repository-names \"${algorithm_name}\" > /dev/null 2>&1\n",
    "\n",
    "if [ $? -ne 0 ]\n",
    "then\n",
    "    aws ecr create-repository --repository-name \"${algorithm_name}\" > /dev/null\n",
    "fi\n",
    "\n",
    "# Get the login command from ECR and execute it directly\n",
    "$(aws ecr get-login --region ${region} --no-include-email) 2> /dev/null\n",
    "\n",
    "# Build the docker image locally with the image name and then push it to ECR\n",
    "# with the full name.\n",
    "\n",
    "docker build  -t ${algorithm_name} . --no-cache # > /dev/null 2>&1\n",
    "docker tag ${algorithm_name} ${fullname} > /dev/null 2>&1\n",
    "\n",
    "docker push ${fullname} #> /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This step runs the training instance (default for GPU is a ml.p3.2xlarge and for the default is CPU is an ml.c4.2xlarge), and begins training with the data specified in `fit()`\n",
    "\n",
    "This section has lots of configurable values\n",
    "You need to change `estimator.fit(...)`:to be the location of the data used for training. (the bucket you uploaded the .tar to) It should be in the format `\"s3://BUCKET-NAME\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-01-02 23:10:08 Starting - Starting the training job...\n",
      "2020-01-02 23:10:10 Starting - Launching requested ML instances......\n",
      "2020-01-02 23:11:16 Starting - Preparing the instances for training...\n",
      "2020-01-02 23:12:09 Downloading - Downloading input data...\n",
      "2020-01-02 23:12:25 Training - Downloading the training image............\n",
      "2020-01-02 23:14:29 Training - Training image download completed. Training in progress.\u001b[34m.\u001b[0m\n",
      "\u001b[34mDownloading model.\u001b[0m\n",
      "\u001b[34mSuccessfully created the TFRecords: /opt/ml/input/data/training/train.record.\u001b[0m\n",
      "\u001b[34mSuccessfully created the TFRecords: /opt/ml/input/data/training/eval.record.\u001b[0m\n",
      "\u001b[34mRecords generated.\u001b[0m\n",
      "\u001b[34mHyperparameters parsed.\u001b[0m\n",
      "\u001b[34mBeginning training on Docker image\u001b[0m\n",
      "\u001b[34mResults of training:\n",
      "    Checkpoint 100 accuracy: 16.795%\n",
      "    Checkpoint 150 accuracy: 19.518%\n",
      "\u001b[0m\n",
      "\u001b[34mCheckpoint 150 will be converted..\u001b[0m\n",
      "\u001b[34mConverting checkpoint to tflite.\u001b[0m\n",
      "\u001b[34mCompiling model for Edge TPU\u001b[0m\n",
      "\n",
      "2020-01-02 23:21:28 Uploading - Uploading generated training model\n",
      "2020-01-02 23:21:54 Completed - Training job completed\n",
      "Training seconds: 585\n",
      "Billable seconds: 585\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "\n",
    "# Uses GPU by default, change to false to use CPU\n",
    "use_gpu = True\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "instance_type = None\n",
    "algorithm_name = None\n",
    "\n",
    "import boto3\n",
    "\n",
    "client = boto3.client('sts')\n",
    "account = client.get_caller_identity()['Account']\n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name\n",
    "\n",
    "if not use_gpu:\n",
    "    instance_type = 'ml.c4.2xlarge'\n",
    "    algorithm_name = 'wpi-cpu'\n",
    "else:\n",
    "    instance_type = 'ml.p3.2xlarge'\n",
    "    algorithm_name = 'wpi-gpu'\n",
    "\n",
    "# The number of epochs to train to. 1000 is a safe number. With the default instance, it should take 45 minutes.\n",
    "hyperparameters = {'epochs': 150,\n",
    "                   'batch_size': 64}\n",
    "\n",
    "ecr_image = '{}.dkr.ecr.{}.amazonaws.com/{}:latest'.format(account, region, algorithm_name)\n",
    "\n",
    "# The estimator object, using our notebook, training instance, the ECR image, and the specified training steps\n",
    "estimator = Estimator(role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type=instance_type,\n",
    "                      image_name=ecr_image,\n",
    "                      hyperparameters=hyperparameters)\n",
    "\n",
    "# Change this bucket if you want to train with your own data. The WPILib bucket contains thousands of high quality labeled images.\n",
    "# s3://wpilib\n",
    "estimator.fit(\"s3://wpilib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
